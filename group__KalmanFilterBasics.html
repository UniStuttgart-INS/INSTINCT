<!-- HTML header for doxygen 1.9.2-->
<!DOCTYPE html
    PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "https://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
    <meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=11" />
    <meta name="generator" content="Doxygen 1.9.8" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>INSTINCT: Basics about Kalman Filtering and how it is used in INSTINCT</title>
    <link href="tabs.css" rel="stylesheet" type="text/css" />
    <script type="text/javascript" src="jquery.js"></script>
    <script type="text/javascript" src="dynsections.js"></script>
    <link href="navtree.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="resize.js"></script>
<script type="text/javascript" src="navtreedata.js"></script>
<script type="text/javascript" src="navtree.js"></script>
    <link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/searchdata.js"></script>
<script type="text/javascript" src="search/search.js"></script>
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:d3d9a9a6595521f9666a5e94cc830dab83b65699&amp;dn=expat.txt MIT */
  $(document).ready(function() { init_search(); });
/* @license-end */
</script>
    <link href="doxygen.css" rel="stylesheet" type="text/css" />
    <link href="customdoxygen.css" rel="stylesheet" type="text/css"/>
    <script type="text/javascript" src="eigen_navtree_hacks.js"></script>
</head>
<body>
        <div id="top">
            <!-- do not remove this div, it is closed by doxygen! -->
            <div id="titlearea">
                <table cellspacing="0" cellpadding="0">
                    <tbody>
                        <tr style="height: 56px;">
                            <td id="projectlogo"><img alt="Logo" src="INSTINCT_Logo_Text.png" /></td>
                            <td id="projectalign" style="padding-left: 0.5em;">
                                <div id="projectname">
                                    <!-- INSTINCT -->
                                    <span id="projectnumber">&#160;1.1.0</span>
                                </div>
                            </td>
                            <td>        <div id="MSearchBox" class="MSearchBoxInactive">
        <span class="left">
          <span id="MSearchSelect"                onmouseover="return searchBox.OnSearchSelectShow()"                onmouseout="return searchBox.OnSearchSelectHide()">&#160;</span>
          <input type="text" id="MSearchField" value="" placeholder="Search" accesskey="S"
               onfocus="searchBox.OnSearchFieldFocus(true)" 
               onblur="searchBox.OnSearchFieldFocus(false)" 
               onkeyup="searchBox.OnSearchFieldChange(event)"/>
          </span><span class="right">
            <a id="MSearchClose" href="javascript:searchBox.CloseResultsWindow()"><img id="MSearchCloseImg" border="0" src="search/close.svg" alt=""/></a>
          </span>
        </div>
</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <!-- end header part -->
<!-- Generated by Doxygen 1.9.8 -->
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:d3d9a9a6595521f9666a5e94cc830dab83b65699&amp;dn=expat.txt MIT */
var searchBox = new SearchBox("searchBox", "search/",'.html');
/* @license-end */
</script>
</div><!-- top -->
<div id="side-nav" class="ui-resizable side-nav-resizable">
  <div id="nav-tree">
    <div id="nav-tree-contents">
      <div id="nav-sync" class="sync"></div>
    </div>
  </div>
  <div id="splitbar" style="-moz-user-select:none;" 
       class="ui-resizable-handle">
  </div>
</div>
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:d3d9a9a6595521f9666a5e94cc830dab83b65699&amp;dn=expat.txt MIT */
$(document).ready(function(){initNavTree('group__KalmanFilterBasics.html',''); initResizable(); });
/* @license-end */
</script>
<div id="doc-content">
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<div id="MSearchResults">
<div class="SRPage">
<div id="SRIndex">
<div id="SRResults"></div>
<div class="SRStatus" id="Loading">Loading...</div>
<div class="SRStatus" id="Searching">Searching...</div>
<div class="SRStatus" id="NoMatches">No Matches</div>
</div>
</div>
</div>
</div>

<div class="header">
  <div class="headertitle"><div class="title">Basics about Kalman Filtering and how it is used in INSTINCT<div class="ingroups"><a class="el" href="group__DataProcessor__chapter.html">DataProcessor</a> &raquo; <a class="el" href="group__DataProcessor__KalmanFilter__chapter.html">KalmanFilter</a></div></div></div>
</div><!--header-->
<div class="contents">
<h1><a class="anchor" id="KalmanFilterBasics-Basic"></a>
Basic relations</h1>
<p>We are going to postulate the following things about the <b>Kalman filter</b></p><ul>
<li>The <b>Kalman filter</b> algorithm directly emerges from Bayes' Thereom (we are going to show this below)</li>
<li>The <b>Linear Kalman filter</b> is the best estimator in case we are dealing only with Gaussian probability density functions</li>
<li>The <b>Linear Kalman filter</b> algorithm consists of two parts - prediction and update/correction - which can be executed continuously one after each other, while carrying the probability density function at any given epoch.</li>
</ul>
<p>For any multivariate Gaussian state of dimension <img class="formulaInl" alt="$n$" src="form_351.svg" width="11" height="11"/>, we can express its Gaussian probability density function as</p>
<p><a class="anchor" id="eq-Gaussian"></a> </p><p class="formulaDsp">
<img class="formulaDsp" alt="\begin{equation} \label{eq:eq-Gaussian}
p(\boldsymbol{x})=\det((2\pi)^n\boldsymbol{\Sigma})^{-\frac{1}{2}}\cdot \exp\left(-\frac{1}{2}(\boldsymbol{x}-\boldsymbol{\mu})^T\boldsymbol{\Sigma}^{-1}(\boldsymbol{x}-\boldsymbol{\mu})\right)
\end{equation}" src="form_352.svg" width="1689" height="42"/>
</p>
<p><img src="KalmanFilterBasics-1DGaussian.png" alt="" width="400" class="inline" title="1D Gaussian distribution"/>     <img src="KalmanFilterBasics-3DGaussian.png" alt="" width="200" class="inline" title="Surface of equal probability of a 3D Gaussian"/>    </p>
<p>The basic idea of the Kalman filter is that both, prediction and update are represented by linear function/transformations between the state-space respectively the observation space. <a class="anchor" id="eq-predictation_and_observation_model"></a> </p><p class="formulaDsp">
<img class="formulaDsp" alt="\begin{equation} \label{eq:eq-predictation_and_observation_model}
\begin{aligned}
\boldsymbol{x}_t&amp;=\boldsymbol{\Phi}_t\boldsymbol{x}_{t-1}+\boldsymbol{B}_t\boldsymbol{u}_t+\boldsymbol{\varepsilon}_t\\
\boldsymbol{z}_t&amp;=\boldsymbol{H}_t\boldsymbol{x}_t+\boldsymbol{\upsilon}_t
\end{aligned}
\end{equation}" src="form_353.svg" width="1570" height="42"/>
</p>
<p> The following table summarizes the meaning of these variables.</p>
<table class="markdownTable">
<tr class="markdownTableHead">
<th class="markdownTableHeadNone">Variable   </th><th class="markdownTableHeadNone">Meaning    </th></tr>
<tr class="markdownTableRowOdd">
<td class="markdownTableBodyNone"><img class="formulaInl" alt="$\boldsymbol{\Phi}_t$" src="form_354.svg" width="19" height="16"/>   </td><td class="markdownTableBodyNone"><img class="formulaInl" alt="$(n\times n)$" src="form_355.svg" width="54" height="19"/> state transition matrix, which describes the linear relation between the state at time <img class="formulaInl" alt="$(t-1)$" src="form_356.svg" width="49" height="19"/> and <img class="formulaInl" alt="$t$" src="form_357.svg" width="7" height="14"/>.    </td></tr>
<tr class="markdownTableRowEven">
<td class="markdownTableBodyNone"><img class="formulaInl" alt="$\boldsymbol{B}_t$" src="form_358.svg" width="21" height="16"/>   </td><td class="markdownTableBodyNone">(linear) control-input model which is applied to the control vector <img class="formulaInl" alt="$ \boldsymbol{u}_t$" src="form_359.svg" width="18" height="12"/> (note: in many cases ) <img class="formulaInl" alt="$ \boldsymbol{u}_t$" src="form_359.svg" width="18" height="12"/> is zero, in particular when we deal with an error-state Kalman filter    </td></tr>
<tr class="markdownTableRowOdd">
<td class="markdownTableBodyNone"><img class="formulaInl" alt="$\boldsymbol{H}_t$" src="form_360.svg" width="25" height="16"/>   </td><td class="markdownTableBodyNone">observation model, which linearly maps the state space into the observation space    </td></tr>
<tr class="markdownTableRowEven">
<td class="markdownTableBodyNone"><img class="formulaInl" alt="$\boldsymbol{\varepsilon}_t$" src="form_361.svg" width="14" height="12"/>   </td><td class="markdownTableBodyNone">process noise, which is assumed to be drawn from a zero mean multivariate normal distribution, with covariance <img class="formulaInl" alt="$ \boldsymbol{Q}_t $" src="form_362.svg" width="21" height="18"/> so that <img class="formulaInl" alt="$ \boldsymbol{\varepsilon}_t \sim \mathcal{N}( \boldsymbol{0},\boldsymbol{Q}_t )$" src="form_363.svg" width="107" height="19"/>    </td></tr>
<tr class="markdownTableRowOdd">
<td class="markdownTableBodyNone"><img class="formulaInl" alt="$\boldsymbol{\upsilon}_t$" src="form_364.svg" width="18" height="12"/>   </td><td class="markdownTableBodyNone">observation noise, which is assumed to be zero mean Gaussian white noise with covariance <img class="formulaInl" alt="$ \boldsymbol{R}_t $" src="form_365.svg" width="21" height="16"/> so that <img class="formulaInl" alt="$ \boldsymbol{\upsilon}_t \sim \mathcal{N}( \boldsymbol{0},\boldsymbol{R}_t )$" src="form_366.svg" width="110" height="19"/>   </td></tr>
</table>
<p>Considering now that we have a linear prediction model described above, one can compute the Gaussian probability density function after predication as <a class="anchor" id="eq-prediction"></a> </p><p class="formulaDsp">
<img class="formulaDsp" alt="\begin{equation} \label{eq:eq-prediction}
    p(\boldsymbol{x}_t\, |\,\boldsymbol{u}_t,\boldsymbol{x}_{t-1})=\det((2\pi)^n\boldsymbol{Q}_t)^{-\frac{1}{2}}\cdot \exp\left(-\frac{1}{2}(\boldsymbol{x}_t-\boldsymbol{\Phi}_t\boldsymbol{x}_{t-1}-\boldsymbol{B}_t\boldsymbol{u}_t)^T\boldsymbol{Q}^{-1}_t(\boldsymbol{x}_t-\boldsymbol{\Phi}_t\boldsymbol{x}_{t-1}-\boldsymbol{B}_t\boldsymbol{u}_t)\right).
\end{equation}" src="form_367.svg" width="1846" height="42"/>
</p>
<p>In a similar way, one can derive the Gaussian probability density function after after making noisy observations as <a class="anchor" id="eq-measurement"></a> </p><p class="formulaDsp">
<img class="formulaDsp" alt="\begin{equation} \label{eq:eq-measurement}
    p(\boldsymbol{z}_t\, |\,\boldsymbol{x}_t)=\det((2\pi)^m\boldsymbol{R}_t)^{-\frac{1}{2}}\cdot \exp\left(-\frac{1}{2}(\boldsymbol{z}_t-\boldsymbol{H}_t\boldsymbol{x}_t)^T\boldsymbol{R}^{-1}_t(\boldsymbol{z}_t-\boldsymbol{H}_t\boldsymbol{x}_t)\right)
\end{equation}" src="form_368.svg" width="1745" height="42"/>
</p>
<p>.</p>
<h2><a class="anchor" id="KalmanFilterBasics-Bayes"></a>
Bayes' Theorem</h2>
<p>Assumption (not proven here): Prediction and update of states with underlying Gaussian distributions lead to Gaussian distributions again. Thus, one can make use of Bayes' Theorem which states that prediction can be understood as convolution of the prior probability density function (PDF), here called "belief" <img class="formulaInl" alt="$ bel(\boldsymbol{x}_t)$" src="form_369.svg" width="51" height="19"/> with the PDF of the prediction. This means, prediction can be understood as <a class="anchor" id="eq-Bayes-prediction"></a> </p><p class="formulaDsp">
<img class="formulaDsp" alt="\begin{equation} \label{eq:eq-Bayes-prediction}
bel^-(\boldsymbol{x}_t)=\int {p(\boldsymbol{x}_t\, |\,\boldsymbol{u}_t,\boldsymbol{x}_{t-1})}\cdot {bel(\boldsymbol{x}_{t-1})}\cdot d\boldsymbol{x}_{t-1}
\end{equation}" src="form_370.svg" width="1652" height="42"/>
</p>
<p> where superscript <img class="formulaInl" alt="${\,}^{-}$" src="form_371.svg" width="11" height="2"/> indicates that we obtain a predicted belief from this equation. Moreover, we know from Bayes' Theorem that observation of a state is equal to multiplication of the PDF of the (predicted belief) with PDF of the observation. This means <a class="anchor" id="eq-Bayes-update"></a> </p><p class="formulaDsp">
<img class="formulaDsp" alt="\begin{equation} \label{eq:eq-Bayes-update}
bel(\boldsymbol{x}_t)^+=\eta\cdot p(\boldsymbol{z}_t\, |\,\boldsymbol{x}_t)\cdot bel^-(\boldsymbol{x}_t)
\end{equation}" src="form_372.svg" width="1598" height="21"/>
</p>
<p> where superscript <img class="formulaInl" alt="${\,}^{+}$" src="form_373.svg" width="12" height="11"/> indicates that we obtain a new belief after making an observation which depends on the state. The factor <img class="formulaInl" alt="$\eta$" src="form_374.svg" width="9" height="14"/> makes sure that the obtained Gaussian fulfills the normalization criteria, i.e. its hyper-volume is equal to <img class="formulaInl" alt="$1.0$" src="form_375.svg" width="23" height="14"/>.</p>
<h2><a class="anchor" id="KalmanFilterBasics-Prediction"></a>
Using Bayes' Theorem for Prediction</h2>
<p>Putting the PDF for prediction into Bayes' Theorem we obtain <a class="anchor" id="eq-Bayes-prediction01"></a> </p><p class="formulaDsp">
<img class="formulaDsp" alt="\begin{equation} \label{eq:eq-Bayes-prediction01}
bel^-(\boldsymbol{x}_t) =\eta\int \exp\left(-\frac{1}{2}(\boldsymbol{x}_t-\boldsymbol{\Phi}_t\boldsymbol{x}_{t-1}-\boldsymbol{B}_t\boldsymbol{u}_t)^T\boldsymbol{Q}^{-1}_t(\boldsymbol{x}_t-\boldsymbol{\Phi}_t\boldsymbol{x}_{t-1}-\boldsymbol{B}_t\boldsymbol{u}_t)\right) \exp\left(-\frac{1}{2}(\boldsymbol{x}_{t-1}-\boldsymbol{\mu}_{t-1})^T\boldsymbol{\Sigma}^{-1}_{t-1}(\boldsymbol{x}_{t-1}-\boldsymbol{\mu}_{t-1})\right)\cdot d\boldsymbol{x}_{t-1}
\end{equation}" src="form_376.svg" width="1967" height="42"/>
</p>
<p> where <img class="formulaInl" alt="$ \boldsymbol{\mu}_{t-1}$" src="form_377.svg" width="35" height="14"/> represents the expectation value of the belief before prediction. This can be formally written as <a class="anchor" id="eq-Bayes-prediction02"></a> </p><p class="formulaDsp">
<img class="formulaDsp" alt="\begin{equation} \label{eq:eq-Bayes-prediction02}
bel^-(\boldsymbol{x}_t)=\eta\int \exp(-L_t)\cdot d\boldsymbol{x}_{t-1}
\end{equation}" src="form_378.svg" width="1596" height="42"/>
</p>
<p> where <a class="anchor" id="eq-Bayes-prediction03"></a> </p><p class="formulaDsp">
<img class="formulaDsp" alt="\begin{equation} \label{eq:eq-Bayes-prediction03}
L_t=\frac{1}{2}(\boldsymbol{x}_t-\boldsymbol{\Phi}_t\boldsymbol{x}_{t-1}-\boldsymbol{B}_t\boldsymbol{u}_t)^T\boldsymbol{Q}^{-1}_t(\boldsymbol{x}_t-\boldsymbol{\Phi}_t\boldsymbol{x}_{t-1}-\boldsymbol{B}_t\boldsymbol{u}_t)
+\frac{1}{2}(\boldsymbol{x}_{t-1}-\boldsymbol{\mu}_{t-1})^T\boldsymbol{\Sigma}^{-1}_{t-1}(\boldsymbol{x}_{t-1}-\boldsymbol{\mu}_{t-1}).
\end{equation}" src="form_379.svg" width="1843" height="39"/>
</p>
<p> The basic idea is now to split <img class="formulaInl" alt="$ L_t$" src="form_380.svg" width="18" height="16"/> so that it contains two terms, from which only one depends on <img class="formulaInl" alt="$\boldsymbol{x}_t$" src="form_381.svg" width="18" height="12"/>, i.e. <a class="anchor" id="eq-Bayes-prediction04"></a> </p><p class="formulaDsp">
<img class="formulaDsp" alt="\begin{equation} \label{eq:eq-Bayes-prediction04}
L_t=L_t(\boldsymbol{x}_{t-1},\boldsymbol{x}_t)+L_t(\boldsymbol{x}_t).
\end{equation}" src="form_382.svg" width="1577" height="19"/>
</p>
<p> Thus we obtain <a class="anchor" id="eq-Bayes-prediction05"></a> </p><p class="formulaDsp">
<img class="formulaDsp" alt="\begin{equation} \label{eq:eq-Bayes-prediction05}
bel^-(\boldsymbol{x}_t)=\eta\int \exp\left \{-L_t(\boldsymbol{x}_{t-1},\boldsymbol{x}_t)-L_t(\boldsymbol{x}_t)\right\}\cdot d\boldsymbol{x}_{t-1}
\end{equation}" src="form_383.svg" width="1673" height="42"/>
</p>
<p> which can be simplified to <a class="anchor" id="eq-Bayes-prediction06"></a> </p><p class="formulaDsp">
<img class="formulaDsp" alt="\begin{equation} \label{eq:eq-Bayes-prediction06}
bel^-(\boldsymbol{x}_t)=\eta\cdot \exp\left \{-L_t(\boldsymbol{x}_t)\right\}\int \exp\left \{-L_t(\boldsymbol{x}_{t-1},\boldsymbol{x}_t)\right\}\cdot d\boldsymbol{x}_{t-1}
\end{equation}" src="form_384.svg" width="1698" height="42"/>
</p>
<p> since the term <img class="formulaInl" alt="$ L_t(\boldsymbol{x}_t)$" src="form_385.svg" width="47" height="19"/> can be extracted from the integral as it does not depend on <img class="formulaInl" alt="$ \boldsymbol{x}_{t-1}$" src="form_386.svg" width="35" height="12"/>. Moreover, as we know that the outcome from this function must be a Gaussian again, we can write this as <a class="anchor" id="eq-Bayes-prediction07"></a> </p><p class="formulaDsp">
<img class="formulaDsp" alt="\begin{equation} \label{eq:eq-Bayes-prediction07}
bel^-(\boldsymbol{x}_t)=\eta'\cdot \exp\left \{-L_t(\boldsymbol{x}_t)\right\}
\end{equation}" src="form_387.svg" width="1584" height="21"/>
</p>
<p> The Gaussian probability density function reaches its highest value for its the expectation value. Thus, if we differentiate the exponent in the Gaussian PDF and set the result equal to zero we find its expectation value. Thus, we find <a class="anchor" id="eq-Bayes-prediction08"></a> </p><p class="formulaDsp">
<img class="formulaDsp" alt="\begin{equation} \label{eq:eq-Bayes-prediction08}
\frac{\partial L_t}{\partial \boldsymbol{x}_{t-1}} = \boldsymbol{0} =-\boldsymbol{\Phi}^T_t\boldsymbol{Q}^{-1}_t(\boldsymbol{x}_t-\boldsymbol{\Phi}_t\boldsymbol{x}_{t-1}-\boldsymbol{B}_t\boldsymbol{u}_t)+\boldsymbol{\Sigma}^{-1}_{t-1}(\boldsymbol{x}_{t-1}-\boldsymbol{\mu}_{t-1})
\end{equation}" src="form_388.svg" width="1729" height="42"/>
</p>
<p> which yields <a class="anchor" id="eq-Bayes-prediction09"></a> </p><p class="formulaDsp">
<img class="formulaDsp" alt="\begin{equation} \label{eq:eq-Bayes-prediction09}
\boldsymbol{\Phi}^T_t\boldsymbol{Q}^{-1}_t(\boldsymbol{x}_t-\boldsymbol{\Phi}_t\boldsymbol{x}_{t-1}-\boldsymbol{B}_t\boldsymbol{u}_t)=\boldsymbol{\Sigma}^{-1}_{t-1}(\boldsymbol{x}_{t-1}-\boldsymbol{\mu}_{t-1})\\[2mm]
\end{equation}" src="form_389.svg" width="1671" height="23"/>
</p>
<p> This can be further re-arranged to <a class="anchor" id="eq-Bayes-prediction10"></a> </p><p class="formulaDsp">
<img class="formulaDsp" alt="\begin{eqnarray} \label{eq:eq-Bayes-prediction10}
\boldsymbol{\Phi}^T_t\boldsymbol{Q}^{-1}_t(\boldsymbol{x}_t-\boldsymbol{B}_t\boldsymbol{u}_t)-\boldsymbol{\Phi}^T_t\boldsymbol{Q}^{-1}_t\boldsymbol{\Phi}_t\boldsymbol{x}_{t-1}=\boldsymbol{\Sigma}^{-1}_{t-1}\boldsymbol{x}_{t-1}-\boldsymbol{\Sigma}^{-1}_{t-1}\boldsymbol{\mu}_{t-1}\\
\boldsymbol{\Phi}^T_t\boldsymbol{Q}^{-1}_t\boldsymbol{\Phi}_t\boldsymbol{x}_{t-1}+\boldsymbol{\Sigma}^{-1}_{t-1}\boldsymbol{x}_{t-1}=\boldsymbol{\Phi}^T_t\boldsymbol{Q}^{-1}_t(\boldsymbol{x}_t-\boldsymbol{B}_t\boldsymbol{u}_t)+\boldsymbol{\Sigma}^{-1}_{t-1}\boldsymbol{\mu}_{t-1}\\
(\boldsymbol{\Phi}^T_t\boldsymbol{Q}^{-1}_t\boldsymbol{\Phi}_t+\boldsymbol{\Sigma}^{-1}_{t-1})\boldsymbol{x}_{t-1}=\boldsymbol{\Phi}^T_t\boldsymbol{Q}^{-1}_t(\boldsymbol{x}_t-\boldsymbol{B}_t\boldsymbol{u}_t)+\boldsymbol{\Sigma}^{-1}_{t-1}\boldsymbol{\mu}_{t-1}
\end{eqnarray}" src="form_390.svg" width="1731" height="81"/>
</p>
<p> Moreover, we know that double differentiation of the exponent provides us with the information matrix / weight matrix, i.e. the inverse of the covariance matrix. Thus, we obtain <a class="anchor" id="eq-Bayes-prediction11"></a> </p><p class="formulaDsp">
<img class="formulaDsp" alt="\begin{equation} \label{eq:eq-Bayes-prediction11}
\frac{\partial^2 L_t}{\partial \boldsymbol{x}^2_{t-1}}=\boldsymbol{\Phi}^T_t\boldsymbol{Q}^{-1}_t\boldsymbol{\Phi}^T_t+\boldsymbol{\Sigma}^{-1}_{t-1}=:\boldsymbol{\Psi}^{-1}_t
\end{equation}" src="form_391.svg" width="1610" height="46"/>
</p>
<p> which we can use to find an alternative notation for the findings before <a class="anchor" id="eq-Bayes-prediction12"></a> </p><p class="formulaDsp">
<img class="formulaDsp" alt="\begin{eqnarray} \label{eq:eq-Bayes-prediction12}
\Leftrightarrow\qquad&amp;\boldsymbol{\Psi}^{-1}_t\boldsymbol{x}_{t-1}=\boldsymbol{\Phi}^T_t\boldsymbol{Q}^{-1}_t(\boldsymbol{x}_t-\boldsymbol{B}_t\boldsymbol{u}_t)+\boldsymbol{\Sigma}^{-1}_{t-1}\boldsymbol{\mu}_{t-1}\\
\Leftrightarrow\qquad&amp;\boldsymbol{x}_{t-1}=\boldsymbol{\Psi}_t\left( \boldsymbol{\Phi}^T_t\boldsymbol{Q}^{-1}_t(\boldsymbol{x}_t-\boldsymbol{B}_t\boldsymbol{u}_t)+\boldsymbol{\Sigma}^{-1}_{t-1}\boldsymbol{\mu}_{t-1} \right)
\end{eqnarray}" src="form_392.svg" width="1692" height="63"/>
</p>
<p> Therefore we can re-write the exponent as <a class="anchor" id="eq-Bayes-prediction13"></a> </p><p class="formulaDsp">
<img class="formulaDsp" alt="\begin{equation} \label{eq:eq-Bayes-prediction13}
L_t(\boldsymbol{x}_{t-1},\boldsymbol{x}_t) =\frac{1}{2}\left \{\boldsymbol{x}_{t-1}- \boldsymbol{\Psi}_t\left( \boldsymbol{\Phi}^T_t\boldsymbol{Q}^{-1}_t(\boldsymbol{x}_t-\boldsymbol{B}_t\boldsymbol{u}_t)+\boldsymbol{\Sigma}^{-1}_{t-1}\boldsymbol{\mu}_{t-1} \right) \right\}^T\boldsymbol{\Psi}^{-1}_t
\left \{\boldsymbol{x}_{t-1}- \boldsymbol{\Psi}_t\left( \boldsymbol{\Phi}^T_t\boldsymbol{Q}^{-1}_t(\boldsymbol{x}_t-\boldsymbol{B}_t\boldsymbol{u}_t)+\boldsymbol{\Sigma}^{-1}_{t-1}\boldsymbol{\mu}_{t-1} \right) \right\}
\end{equation}" src="form_393.svg" width="1937" height="37"/>
</p>
<p> Now we are able to compute the term <img class="formulaInl" alt="$ L_t$" src="form_380.svg" width="18" height="16"/>which only one depends on <img class="formulaInl" alt="$\boldsymbol{x}_t$" src="form_381.svg" width="18" height="12"/> <a class="anchor" id="eq-Bayes-prediction14"></a> </p><p class="formulaDsp">
<img class="formulaDsp" alt="\begin{equation} \label{eq:eq-Bayes-prediction14}
L_t(\boldsymbol{x}_t)=L_t-L_t(\boldsymbol{x}_{t-1},\boldsymbol{x}_t)
=\frac{1}{2}(\boldsymbol{x}_t-\boldsymbol{\Phi}_t\boldsymbol{x}_{t-1}-\boldsymbol{B}_t\boldsymbol{u}_t)^T\boldsymbol{Q}^{-1}_t(\boldsymbol{x}_t-\boldsymbol{\Phi}_t\boldsymbol{x}_{t-1}-\boldsymbol{B}_t\boldsymbol{u}_t)
+\frac{1}{2}(\boldsymbol{x}_{t-1}-\boldsymbol{\mu}_{t-1})^T\boldsymbol{\Sigma}^{-1}_{t-1}(\boldsymbol{x}_{t-1}-\boldsymbol{\mu}_{t-1})
-\frac{1}{2}\left \{\boldsymbol{x}_{t-1}- \boldsymbol{\Psi}_t\left( \boldsymbol{\Phi}^T_t\boldsymbol{Q}^{-1}_t(\boldsymbol{x}_t-\boldsymbol{B}_t\boldsymbol{u}_t)+\boldsymbol{\Sigma}^{-1}_{t-1}\boldsymbol{\mu}_{t-1} \right) \right\}^T\boldsymbol{\Psi}^{-1}_t
\left \{\boldsymbol{x}_{t-1}- \boldsymbol{\Psi}_t\left( \boldsymbol{\Phi}^T_t\boldsymbol{Q}^{-1}_t(\boldsymbol{x}_t-\boldsymbol{B}_t\boldsymbol{u}_t)+\boldsymbol{\Sigma}^{-1}_{t-1}\boldsymbol{\mu}_{t-1} \right) \right\}
\end{equation}" src="form_394.svg" width="2350" height="37"/>
</p>
<p> This leads to <a class="anchor" id="eq-Bayes-prediction15"></a> </p><p class="formulaDsp">
<img class="formulaDsp" alt="\begin{equation} \label{eq:eq-Bayes-prediction15}
L_t(\boldsymbol{x}_t)=\underline{\underline{\frac{1}{2}\boldsymbol{x}^T_{t-1}\boldsymbol{\Phi}^T_t\boldsymbol{Q}^{-1}_t\boldsymbol{\Phi}_t\boldsymbol{x}_{t-1}}}\underline{-\boldsymbol{x}^T_{t-1}\boldsymbol{\Phi}^T_t\boldsymbol{Q}^{-1}_t(\boldsymbol{x}_t-\boldsymbol{B}_t\boldsymbol{u}_t)}
+\frac{1}{2}(\boldsymbol{x}_t-\boldsymbol{B}_t\boldsymbol{u}_t)^T\boldsymbol{Q}^{-1}_t(\boldsymbol{x}_t-\boldsymbol{B}_t\boldsymbol{u}_t)
\underline{\underline{+\frac{1}{2}\boldsymbol{x}^T_{t-1}\boldsymbol{\Sigma}^{-1}_{t-1}\boldsymbol{x}_{t-1}}}\underline{-\boldsymbol{x}^T_{t-1}\boldsymbol{\Sigma}^{-1}_{t-1}\boldsymbol{\mu}_{t-1}}+\frac{1}{2}\boldsymbol{\mu}^T_{t-1}\boldsymbol{\Sigma}^{-1}_{t-1}\boldsymbol{\mu}_{t-1}
\underline{\underline{-\frac{1}{2}\boldsymbol{x}^T_{t-1}(\boldsymbol{\Phi}^T_t\boldsymbol{Q}^{-1}_t\boldsymbol{\Phi}_t+\boldsymbol{\Sigma}^{-1}_{t-1})\boldsymbol{x}_{t-1}}}
\underline{+\boldsymbol{x}^T_{t-1}\left(\boldsymbol{\Phi}^T_t\boldsymbol{Q}^{-1}_t(\boldsymbol{x}_t-\boldsymbol{B}_t\boldsymbol{u}_t)+\boldsymbol{\Sigma}^{-1}_{t-1}\boldsymbol{\mu}_{t-1}\right)}
-\frac{1}{2}\left( \boldsymbol{\Phi}^T_t\boldsymbol{Q}^{-1}_t(\boldsymbol{x}_t-\boldsymbol{B}_t\boldsymbol{u}_t)+\boldsymbol{\Sigma}^{-1}_{t-1}\boldsymbol{\mu}_{t-1} \right)^T(\boldsymbol{\Phi}^T_t\boldsymbol{Q}^{-1}_t\boldsymbol{\Phi}_t+\boldsymbol{\Sigma}^{-1}_{t-1})^{-1}
\left( \boldsymbol{\Phi}^T_t\boldsymbol{Q}^{-1}_t(\boldsymbol{x}_t-\boldsymbol{B}_t\boldsymbol{u}_t)+\boldsymbol{\Sigma}^{-1}_{t-1}\boldsymbol{\mu}_{t-1} \right)
\end{equation}" src="form_395.svg" width="2693" height="44"/>
</p>
<p> which simplifies to <a class="anchor" id="eq-Bayes-prediction16"></a> </p><p class="formulaDsp">
<img class="formulaDsp" alt="\begin{equation} \label{eq:eq-Bayes-prediction16}
L_t(\boldsymbol{x}_t)=\frac{1}{2}(\boldsymbol{x}_t-\boldsymbol{B}_t\boldsymbol{u}_t)^T\boldsymbol{Q}^{-1}_t(\boldsymbol{x}_t-\boldsymbol{B}_t\boldsymbol{u}_t)+\frac{1}{2}\boldsymbol{\mu}^T_{t-1}\boldsymbol{\Sigma}^{-1}_{t-1}\boldsymbol{\mu}_{t-1}-\frac{1}{2}\left( \boldsymbol{\Phi}^T_t\boldsymbol{Q}^{-1}_t(\boldsymbol{x}_t-\boldsymbol{B}_t\boldsymbol{u}_t)+\boldsymbol{\Sigma}^{-1}_{t-1}\boldsymbol{\mu}_{t-1} \right)^T(\boldsymbol{\Phi}^T_t\boldsymbol{Q}^{-1}_t\boldsymbol{\Phi}_t+\boldsymbol{\Sigma}^{-1}_{t-1})^{-1} \left( \boldsymbol{\Phi}^T_t\boldsymbol{Q}^{-1}_t(\boldsymbol{x}_t-\boldsymbol{B}_t\boldsymbol{u}_t)+\boldsymbol{\Sigma}^{-1}_{t-1}\boldsymbol{\mu}_{t-1} \right)
\end{equation}" src="form_396.svg" width="2084" height="37"/>
</p>
<p> Based on this, we can differentiate w.r.t. to <img class="formulaInl" alt="$\boldsymbol{x}_t$" src="form_381.svg" width="18" height="12"/> in order to obtain the expectation value, i.e. <a class="anchor" id="eq-Bayes-prediction17"></a> </p><p class="formulaDsp">
<img class="formulaDsp" alt="\begin{equation} \label{eq:eq-Bayes-prediction17}
\frac{\partial L_t(\boldsymbol{x}_t)}{\partial \boldsymbol{x}_t}= \boldsymbol{0} = \boldsymbol{Q}^{-1}_t(\boldsymbol{x}_t-\boldsymbol{B}_t\boldsymbol{u}_t)-\boldsymbol{Q}^{-1}_t\boldsymbol{\Phi}_t(\boldsymbol{\Phi}^T_t\boldsymbol{Q}^{-1}_t\boldsymbol{\Phi}_t+\boldsymbol{\Sigma}^{-1}_{t-1})^{-1}
\left( \boldsymbol{\Phi}^T_t\boldsymbol{Q}^{-1}_t(\boldsymbol{x}_t-\boldsymbol{B}_t\boldsymbol{u}_t)+\boldsymbol{\Sigma}^{-1}_{t-1}\boldsymbol{\mu}_{t-1} \right)
\end{equation}" src="form_397.svg" width="1853" height="40"/>
</p>
<p> respectively, <a class="anchor" id="eq-Bayes-prediction18"></a> </p><p class="formulaDsp">
<img class="formulaDsp" alt="\begin{equation} \label{eq:eq-Bayes-prediction18}
\boldsymbol{0}=\left(\boldsymbol{Q}^{-1}_t-\boldsymbol{Q}^{-1}_t\boldsymbol{\Phi}_t(\boldsymbol{\Phi}^T_t\boldsymbol{Q}^{-1}_t\boldsymbol{\Phi}_t+\boldsymbol{\Sigma}^{-1}_{t-1})^{-1} \boldsymbol{\Phi}^T_t\boldsymbol{Q}^{-1}_t\right)(\boldsymbol{x}_t-\boldsymbol{B}_t\boldsymbol{u}_t)
\end{equation}" src="form_398.svg" width="1717" height="33"/>
</p>
<p> If we use the matrix lemma <a class="anchor" id="eq-Bayes-prediction19"></a> </p><p class="formulaDsp">
<img class="formulaDsp" alt="\begin{equation} \label{eq:eq-Bayes-prediction19}
\boldsymbol{Q}^{-1}_t-\boldsymbol{Q}^{-1}_t\boldsymbol{\Phi}_t(\boldsymbol{\Phi}^T_t\boldsymbol{Q}^{-1}_t\boldsymbol{\Phi}_t+\boldsymbol{\Sigma}^{-1}_{t-1})^{-1} \boldsymbol{\Phi}^T_t\boldsymbol{Q}^{-1}_t=(\boldsymbol{Q}_t+\boldsymbol{\Phi}_t\boldsymbol{\Sigma}_{t-1}\boldsymbol{\Phi}^T_t)^{-1}\\[6mm]
\end{equation}" src="form_399.svg" width="1734" height="23"/>
</p>
<p> We can further simplify and obtain <a class="anchor" id="eq-Bayes-prediction20"></a> </p><p class="formulaDsp">
<img class="formulaDsp" alt="\begin{equation} \label{eq:eq-Bayes-prediction20}
\frac{\partial L_t(\boldsymbol{x}_t)}{\partial \boldsymbol{x}_t}=\boldsymbol{0}=
(\boldsymbol{Q}_t+\boldsymbol{\Phi}_t\boldsymbol{\Sigma}_{t-1}\boldsymbol{\Phi}^T_t)^{-1}(\boldsymbol{x}_t-\boldsymbol{B}_t\boldsymbol{u}_t)
-\boldsymbol{Q}^{-1}_t\boldsymbol{\Phi}_t(\boldsymbol{\Phi}^T_t\boldsymbol{Q}^{-1}_t\boldsymbol{\Phi}_t+\boldsymbol{\Sigma}^{-1}_{t-1})^{-1}\boldsymbol{\Sigma}^{-1}_{t-1}\boldsymbol{\mu}_{t-1}
\end{equation}" src="form_400.svg" width="1818" height="40"/>
</p>
<p> which turns out to read as <a class="anchor" id="eq-Bayes-prediction21"></a> </p><p class="formulaDsp">
<img class="formulaDsp" alt="\begin{equation} \label{eq:eq-Bayes-prediction21}
(\boldsymbol{Q}_t+\boldsymbol{\Phi}_t\boldsymbol{\Sigma}_{t-1}\boldsymbol{\Phi}^T_t)^{-1}(\boldsymbol{x}_t-\boldsymbol{B}_t\boldsymbol{u}_t)=\boldsymbol{Q}^{-1}_t\boldsymbol{\Phi}_t(\boldsymbol{\Phi}^T_t\boldsymbol{Q}^{-1}_t\boldsymbol{\Phi}_t+\boldsymbol{\Sigma}^{-1}_{t-1})^{-1}\boldsymbol{\Sigma}^{-1}_{t-1}\boldsymbol{\mu}_{t-1}
\end{equation}" src="form_401.svg" width="1759" height="23"/>
</p>
<p> This can be written also as <a class="anchor" id="eq-Bayes-prediction22"></a> </p><p class="formulaDsp">
<img class="formulaDsp" alt="\begin{eqnarray} \label{eq:eq-Bayes-prediction22}
\boldsymbol{x}_t&amp;=\boldsymbol{B}_t\boldsymbol{u}_t+\underset{\boldsymbol{\Phi}_t+\boldsymbol{\Phi}_t\boldsymbol{\Sigma}_{t-1}\boldsymbol{\Phi}^T_t\boldsymbol{Q}^{-1}_t\boldsymbol{\Phi}_t}{\underbrace{(\boldsymbol{Q}_t+\boldsymbol{\Phi}_t\boldsymbol{\Sigma}_{t-1}\boldsymbol{\Phi}^T_t)^{-1}\boldsymbol{Q}^{-1}_t\boldsymbol{\Phi}_t}}\cdot              \underset{(\boldsymbol{\Sigma}_{t-1}\boldsymbol{\Phi}^T_t\boldsymbol{Q}^{-1}_t\boldsymbol{\Phi}_t+\boldsymbol{I})^{-1}}{\underbrace{(\boldsymbol{\Phi}^T_t\boldsymbol{Q}^{-1}_t\boldsymbol{\Phi}_t+\boldsymbol{\Sigma}^{-1}_{t-1})^{-1}\boldsymbol{\Sigma}^{-1}_{t-1}}}\boldsymbol{\mu}_{t-1}\notag\\
&amp;=\boldsymbol{B}_t\boldsymbol{u}_t+\boldsymbol{\Phi}_t\cdot\underset{=\boldsymbol{I}}{\underbrace{(\boldsymbol{I}+\boldsymbol{\Sigma}_{t-1}\boldsymbol{\Phi}^T_t\boldsymbol{Q}^{-1}_t\boldsymbol{\Phi}_t)(\boldsymbol{\Sigma}_{t-1}\boldsymbol{\Phi}^T_t\boldsymbol{Q}^{-1}_t\boldsymbol{\Phi}_t+\boldsymbol{I})^{-1}}}\cdot\boldsymbol{\mu}_{t-1}\notag\\
&amp;=\boldsymbol{B}_t\boldsymbol{u}_t+\boldsymbol{\Phi}_t\boldsymbol{\mu}_{t-1}
\end{eqnarray}" src="form_402.svg" width="1775" height="133"/>
</p>
<p> We also find the covariance after prediction, from the second derivative, i.e. <a class="anchor" id="eq-Bayes-prediction23"></a> </p><p class="formulaDsp">
<img class="formulaDsp" alt="\begin{equation} \label{eq:eq-Bayes-prediction23}
\frac{\partial^2 L_t(\boldsymbol{x}_t)}{\partial \boldsymbol{x}^2_t}=(\boldsymbol{Q}_t+\boldsymbol{\Phi}_t\boldsymbol{\Sigma}_{t-1}\boldsymbol{\Phi}^T_t)^{-1} \quad \Rightarrow \quad \bar{\boldsymbol{\Sigma}}_t^{-1} = (\boldsymbol{Q}_t+\boldsymbol{\Phi}_t\boldsymbol{\Sigma}_{t-1}\boldsymbol{\Phi}^T_t)^{-1}
\end{equation}" src="form_403.svg" width="1736" height="46"/>
</p>
<p> Thus, in summary we could prove that the prediction step in the linear Kalman filter turns out to be rather simple, ie. <a class="anchor" id="eq-Bayes-prediction_final"></a> </p><p class="formulaDsp">
<img class="formulaDsp" alt="\begin{eqnarray} \label{eq:eq-Bayes-prediction_final}
\boldsymbol{\mu}_t^-&amp;=\boldsymbol{\Phi}_t\boldsymbol{\mu}_{t-1}+\boldsymbol{B}_t\boldsymbol{u}_t\\
\boldsymbol{\Sigma}_t^-&amp;=\boldsymbol{\Phi}_t\boldsymbol{\Sigma}_{t-1}\boldsymbol{\Phi}^T_t+\boldsymbol{Q}_t
\end{eqnarray}" src="form_404.svg" width="1575" height="47"/>
</p>
<p> where the superscript <img class="formulaInl" alt="$-$" src="form_405.svg" width="12" height="2"/> indicates, that we deal with the predicted state and the covariance, respectively.</p>
<h2><a class="anchor" id="KalmanFilterBasics-Update"></a>
Using Bayes' Theorem for Update/Correction</h2>
<p>As for the update step (we will call it ''update'' hereafter although one might also see it as ''correction'' step), we start with Bayes' Theorem that tell us the conditional likelihood in case we observe an uncertain state. This can be written as <a class="anchor" id="eq-Bayes-update01"></a> </p><p class="formulaDsp">
<img class="formulaDsp" alt="\begin{equation} \label{eq:eq-Bayes-update01}
bel(\boldsymbol{x}_t)=\boldsymbol{\eta}\cdot\underset{\sim\mathcal{N}(\boldsymbol{z}_t;\boldsymbol{H}_t\boldsymbol{x}_t,\boldsymbol{R}_t)}{\underbrace{p(\boldsymbol{z}_t\, |\,\boldsymbol{x}_t)}}\cdot \underset{\sim\mathcal{N}(\boldsymbol{x}_t;\boldsymbol{\mu}_t^{-},\boldsymbol{\Sigma}_t^{-})}{\underbrace{\overline{bel}(\boldsymbol{x}_t)}}
\end{equation}" src="form_406.svg" width="1633" height="51"/>
</p>
<p> where <img class="formulaInl" alt="$\boldsymbol{\eta}$" src="form_407.svg" width="11" height="14"/> is a factor that normalizes the product on the right side, so that a proper PDF is obtained. Knowing know that everything is Gaussian, including the term on the left side, we can write above's equation as <a class="anchor" id="eq-Bayes-update02"></a> </p><p class="formulaDsp">
<img class="formulaDsp" alt="\begin{equation} \label{eq:eq-Bayes-update02}
bel(\boldsymbol{x}_t)=\boldsymbol{\eta}\cdot exp\{-\boldsymbol{J}_t\}
\end{equation}" src="form_408.svg" width="1559" height="19"/>
</p>
<p> where <a class="anchor" id="eq-Bayes-update03"></a> </p><p class="formulaDsp">
<img class="formulaDsp" alt="\begin{equation} \label{eq:eq-Bayes-update03}
\boldsymbol{J}_t=\frac{1}{2}(\boldsymbol{z}_t-\boldsymbol{H}_t\boldsymbol{x}_t)^T\boldsymbol{R}^{-1}_t(\boldsymbol{z}_t-\boldsymbol{H}_t\boldsymbol{x}_t)+\frac{1}{2}(\boldsymbol{x}_t-\boldsymbol{\mu}_t^{-})^T(\boldsymbol{\Sigma}_t^{-})^{-1}(\boldsymbol{x}_t-\boldsymbol{\mu}_t^{-})
\end{equation}" src="form_409.svg" width="1745" height="39"/>
</p>
<p> Like before, we can find the expectation value of this function, by differentiating the term <img class="formulaInl" alt="$\boldsymbol{J}_t$" src="form_410.svg" width="18" height="16"/> and setting the result equal to zero.This means <a class="anchor" id="eq-Bayes-update04"></a> </p><p class="formulaDsp">
<img class="formulaDsp" alt="\begin{equation} \label{eq:eq-Bayes-update04}
\frac{\partial\boldsymbol{J}_t}{\partial \boldsymbol{x}_t}=\boldsymbol{0} = -\boldsymbol{H}^T_t\boldsymbol{R}^{-1}_t(\boldsymbol{z}_t-\boldsymbol{H}_t\boldsymbol{x}_t)+(\boldsymbol{\Sigma}_t^{-})^{-1}(\boldsymbol{x}_t-\boldsymbol{\mu}_t^{-})
\end{equation}" src="form_411.svg" width="1680" height="42"/>
</p>
<p> which can be rearranged to <a class="anchor" id="eq-Bayes-update05"></a> </p><p class="formulaDsp">
<img class="formulaDsp" alt="\begin{equation} \label{eq:eq-Bayes-update05}
\boldsymbol{H}^T_t\boldsymbol{R}^{-1}_t(\boldsymbol{z}_t-\boldsymbol{H}_t\boldsymbol{\mu}_t^{+})=(\boldsymbol{\Sigma}_t^{-})^{-1}(\boldsymbol{\mu}_t^{+}-\boldsymbol{\mu}_t^{-})
\end{equation}" src="form_412.svg" width="1636" height="23"/>
</p>
<p> The second derivative of <img class="formulaInl" alt="$\boldsymbol{J}_t$" src="form_410.svg" width="18" height="16"/> provides us the inverse of the covariance matrix, i.e. <a class="anchor" id="eq-Bayes-update06"></a> </p><p class="formulaDsp">
<img class="formulaDsp" alt="\begin{equation} \label{eq:eq-Bayes-update06}
\frac{\partial^2 \boldsymbol{J}_t}{\partial \boldsymbol{x}^2_t}=\boldsymbol{\Sigma}_t^{-1}
\boldsymbol{H}^T_t\boldsymbol{R}^{-1}_t\boldsymbol{H}_t+(\boldsymbol{\Sigma}_t^{-})^{-1}
\end{equation}" src="form_413.svg" width="1603" height="46"/>
</p>
<p> which can be rewritten as <a class="anchor" id="eq-Bayes-update07"></a> </p><p class="formulaDsp">
<img class="formulaDsp" alt="\begin{equation} \label{eq:eq-Bayes-update07}
\boldsymbol{\Sigma}_t=(\boldsymbol{H}^T_t\boldsymbol{R}^{-1}_t\boldsymbol{H}_t+(\boldsymbol{\Sigma}_t^{-})^{-1})^{-1}\\[2mm] % 3.37
\end{equation}" src="form_414.svg" width="1594" height="23"/>
</p>
<p> The equation that hold the expectation value can be re-written to <a class="anchor" id="eq-Bayes-update08"></a> </p><p class="formulaDsp">
<img class="formulaDsp" alt="\begin{eqnarray} \label{eq:eq-Bayes-update08}
&amp;\boldsymbol{H}^T_t\boldsymbol{R}^{-1}_t(\boldsymbol{z}_t-\boldsymbol{H}_t\boldsymbol{\mu}_t^{+})\\
&amp;=\boldsymbol{H}^T_t\boldsymbol{R}^{-1}_t(\boldsymbol{z}_t-\boldsymbol{H}_t\boldsymbol{\mu}_t^{+}+\boldsymbol{H}_t\boldsymbol{\mu}_t^{-}-\boldsymbol{H}_t\boldsymbol{\mu}_t^{-})\\
&amp;=\boldsymbol{H}^T_t\boldsymbol{R}^{-1}_t(\boldsymbol{z}_t-\boldsymbol{H}_t\boldsymbol{\mu}_t^{-})-\boldsymbol{H}^T_t\boldsymbol{R}^{-1}_t\boldsymbol{H}_t(\boldsymbol{\mu}_t^{+}-\boldsymbol{\mu}_t^{-})
\end{eqnarray}" src="form_415.svg" width="1659" height="75"/>
</p>
<p> which in turn allows us to identify <a class="anchor" id="eq-Bayes-update09"></a> </p><p class="formulaDsp">
<img class="formulaDsp" alt="\begin{equation} \label{eq:eq-Bayes-update09}
\boldsymbol{H}^T_t\boldsymbol{R}^{-1}_t(\boldsymbol{z}_t-\boldsymbol{H}_t\boldsymbol{\mu}_t^{-})=\underset{=\boldsymbol{\Sigma}^{-1}_t}{\underbrace{(\boldsymbol{H}^T_t\boldsymbol{R}^{-1}_t\boldsymbol{H}_t+(\boldsymbol{\Sigma}_t^{-})^{-1})}}(\boldsymbol{\mu}_t^{+}-\boldsymbol{\mu}_t^{-})
\end{equation}" src="form_416.svg" width="1698" height="53"/>
</p>
<p> Thus, we can write <a class="anchor" id="eq-Bayes-update10"></a> </p><p class="formulaDsp">
<img class="formulaDsp" alt="\begin{equation} \label{eq:eq-Bayes-update10}
\boldsymbol{\Sigma}_t\boldsymbol{H}^T_t\boldsymbol{R}^{-1}_t(\boldsymbol{z}_t-\boldsymbol{H}_t\boldsymbol{\mu}_t^{-})=\boldsymbol{\mu}_t-\boldsymbol{\mu}_t^{-}
\end{equation}" src="form_417.svg" width="1608" height="23"/>
</p>
<p>, or by using the so-called Kalman gain <a class="anchor" id="eq-Bayes-update11"></a> </p><p class="formulaDsp">
<img class="formulaDsp" alt="\begin{equation} \label{eq:eq-Bayes-update11}
\boldsymbol{K}_t=\boldsymbol{\Sigma}_t\boldsymbol{H}^T_t\boldsymbol{R}^{-1}_t
\end{equation}" src="form_418.svg" width="1538" height="23"/>
</p>
<p> we obtain a very simple rule how the update has to be computed <a class="anchor" id="eq-Bayes-update12"></a> </p><p class="formulaDsp">
<img class="formulaDsp" alt="\begin{equation} \label{eq:eq-Bayes-update12}
\boldsymbol{\mu}_t^{+}=\boldsymbol{\mu}_t^{-}+\boldsymbol{K}_t(\boldsymbol{z}_t-\boldsymbol{H}_t\boldsymbol{\mu}_t^{-}) %3.
\end{equation}" src="form_419.svg" width="1580" height="21"/>
</p>
<p> However, we might want a more straightforward expression for the Kalman gain, that contains only the predicted covariance and not the one after making observations. Thus, we do a series of mathematical tricks and obtain <a class="anchor" id="eq-Bayes-update13"></a> </p><p class="formulaDsp">
<img class="formulaDsp" alt="\begin{eqnarray} \label{eq:eq-Bayes-update13}
\boldsymbol{K}_t&amp;=\boldsymbol{\Sigma}_t\boldsymbol{H}^T_t\boldsymbol{R}^{-1}_t\\
&amp;=\boldsymbol{\Sigma}_t\boldsymbol{H}^T_t\boldsymbol{R}^{-1}_t\underset{=\boldsymbol{I}}{\underbrace{(\boldsymbol{H}_t\boldsymbol{\Sigma}_t^{-}\boldsymbol{H}^T_t+\boldsymbol{R}_t)(\boldsymbol{H}_t\boldsymbol{\Sigma}_t^{-}\boldsymbol{H}^T_t+\boldsymbol{R}_t)^{-1}}}\\
&amp;=\boldsymbol{\Sigma}_t(\boldsymbol{H}^T_t\boldsymbol{R}^{-1}_t\boldsymbol{H}_t\boldsymbol{\Sigma}_t^{-}\boldsymbol{H}^T_t+\boldsymbol{H}^T_t\underset{=\boldsymbol{I}}{\underbrace{\boldsymbol{R}^{-1}_t\boldsymbol{R}_t}})(\boldsymbol{H}_t\boldsymbol{\Sigma}_t^{-}\boldsymbol{H}^T_t+\boldsymbol{R}_t)^{-1}\\
&amp;=\boldsymbol{\Sigma}_t(\boldsymbol{H}^T_t\boldsymbol{R}^{-1}_t\boldsymbol{H}_t\boldsymbol{\Sigma}_t^{-}\boldsymbol{H}^T_t+\boldsymbol{H}^T_t)(\boldsymbol{H}_t\boldsymbol{\Sigma}_t^{-}\boldsymbol{H}^T_t+\boldsymbol{R}_t)^{-1}\\
&amp;=\boldsymbol{\Sigma}_t(\boldsymbol{H}^T_t\boldsymbol{R}^{-1}_t\boldsymbol{H}_t\boldsymbol{\Sigma}_t^{-}\boldsymbol{H}^T_t+\underset{=\boldsymbol{I}}{\underbrace{(\boldsymbol{\Sigma}_t^{-})^{-1}\boldsymbol{\Sigma}_t^{-}}}\boldsymbol{H}^T_t)(\boldsymbol{H}_t\boldsymbol{\Sigma}_t^{-}\boldsymbol{H}^T_t+\boldsymbol{R}_t)^{-1}\\
&amp;=\boldsymbol{\Sigma}_t\underset{=\boldsymbol{\Sigma}^{-1}_t}{\underbrace{(\boldsymbol{H}^T_t\boldsymbol{R}^{-1}_t\boldsymbol{H}_t+(\boldsymbol{\Sigma}_t^{-})^{-1})}}\boldsymbol{\Sigma}_t^{-}\boldsymbol{H}^T_t(\boldsymbol{H}_t\boldsymbol{\Sigma}_t^{-}\boldsymbol{H}^T_t+\boldsymbol{R}_t)^{-1}\\
&amp;=\underset{=\boldsymbol{I}}{\underbrace{\boldsymbol{\Sigma}_t\boldsymbol{\Sigma}^{-1}_t}}\boldsymbol{\Sigma}_t^{-}\boldsymbol{H}^T_t(\boldsymbol{H}_t\boldsymbol{\Sigma}_t^{-}\boldsymbol{H}^T_t+\boldsymbol{R}_t)^{-1}\\
&amp;=\boldsymbol{\Sigma}_t^{-}\boldsymbol{H}^T_t(\boldsymbol{H}_t\boldsymbol{\Sigma}_t^{-}\boldsymbol{H}^T_t+\boldsymbol{R}_t)^{-1}
\end{eqnarray}" src="form_421.svg" width="1745" height="347"/>
</p>
<p> We can also find an easier expression for the updated covariance matrix, if we use a matrix lemma and reformulate <a class="anchor" id="eq-Bayes-update14"></a> </p><p class="formulaDsp">
<img class="formulaDsp" alt="\begin{equation} \label{eq:eq-Bayes-update14}
((\boldsymbol{\Sigma}_t^{-})^{-1}+\boldsymbol{H}^T_t\boldsymbol{R}^{-1}_t\boldsymbol{H}_t)^{-1}=\boldsymbol{\Sigma}_t^{-}-\boldsymbol{\Sigma}_t^{-}\boldsymbol{H}^T_t(\boldsymbol{R}_t+\boldsymbol{H}_t\boldsymbol{\Sigma}_t^{-}\boldsymbol{H}^T_t)^{-1}\boldsymbol{H}^T_t\boldsymbol{\Sigma}_t^{-}
\end{equation}" src="form_424.svg" width="1741" height="23"/>
</p>
<p> With that we obtain <a class="anchor" id="eq-Bayes-update15"></a> </p><p class="formulaDsp">
<img class="formulaDsp" alt="\begin{eqnarray} \label{eq:eq-Bayes-update15}
\boldsymbol{\Sigma}_t&amp;=(\boldsymbol{H}^T_t\boldsymbol{R}^{-1}_t\boldsymbol{H}_t+(\boldsymbol{\Sigma}_t^{-})^{-1})^{-1}\\
&amp;=\boldsymbol{\Sigma}_t^{-}-\boldsymbol{\Sigma}_t^{-}\boldsymbol{H}^T_t(\boldsymbol{R}_t+\boldsymbol{H}_t\boldsymbol{\Sigma}_t^{-}\boldsymbol{H}^T_t)^{-1}\boldsymbol{H}_t\boldsymbol{\Sigma}_t^{-}\\
&amp;=[\boldsymbol{I}-\underset{=\boldsymbol{K}_t}{\underbrace{\boldsymbol{\Sigma}_t^{-}\boldsymbol{H}^T_t(\boldsymbol{R}_t+\boldsymbol{H}_t\boldsymbol{\Sigma}_t^{-}\boldsymbol{H}^T_t)^{-1}}}\boldsymbol{H}_t]\boldsymbol{\Sigma}_t^{-}\\
&amp;=(\boldsymbol{I}-\boldsymbol{K}_t\boldsymbol{H}_t)\boldsymbol{\Sigma}_t^{-}
\end{eqnarray}" src="form_427.svg" width="1664" height="130"/>
</p>
<p>Putting everything together we find for the update step the following very simple rules <a class="anchor" id="eq-Bayes-update-final"></a> </p><p class="formulaDsp">
<img class="formulaDsp" alt="\begin{eqnarray} \label{eq:eq-Bayes-update-final}
\boldsymbol{K}_t&amp;=\boldsymbol{\Sigma}_t^{-}\boldsymbol{H}^T_t(\boldsymbol{H}_t\boldsymbol{\Sigma}_t^{-}\boldsymbol{H}^T_t+\boldsymbol{R}_t)^{-1}\\
\boldsymbol{\mu}_t^{+}&amp;=\boldsymbol{\mu}_t^{-}+\boldsymbol{K}_t(\boldsymbol{z}_t-\boldsymbol{H}_t\boldsymbol{\mu}_t^{-}) \\
\boldsymbol{\Sigma}_t&amp;=(\boldsymbol{I}-\boldsymbol{K}_t\boldsymbol{H}_t)\boldsymbol{\Sigma}_t^{-}
\end{eqnarray}" src="form_429.svg" width="1617" height="75"/>
</p>
<h2><a class="anchor" id="KalmanFilterBasics-Summary"></a>
Kalman filtering at a glance</h2>
<p>Assuming that we sequentially make predictions o <a class="anchor" id="eq-Bayes-summary01"></a> </p><p class="formulaDsp">
<img class="formulaDsp" alt="\begin{eqnarray} \label{eq:eq-Bayes-summary01}
\boldsymbol{\mu}_t^-&amp;=\boldsymbol{\Phi}_t\boldsymbol{\mu}_{t-1}^{+}+\boldsymbol{B}_t\boldsymbol{u}_t\\
\boldsymbol{\Sigma}_t^-&amp;=\boldsymbol{\Phi}_t\boldsymbol{\Sigma}_{t-1}^{+}\boldsymbol{\Phi}^T_t+\boldsymbol{Q}_t
\end{eqnarray}" src="form_430.svg" width="1575" height="51"/>
</p>
<p> and update the state and its covariance <a class="anchor" id="eq-Bayes-summary02"></a> </p><p class="formulaDsp">
<img class="formulaDsp" alt="\begin{eqnarray} \label{eq:eq-Bayes-summary02}
\boldsymbol{K}_t&amp;=\boldsymbol{\Sigma}_t^{-}\boldsymbol{H}^T_t(\boldsymbol{H}_t\boldsymbol{\Sigma}_t^{-}\boldsymbol{H}^T_t+\boldsymbol{R}_t)^{-1}\\
\boldsymbol{\mu}_t^{+}&amp;=\boldsymbol{\mu}_t^{-}+\boldsymbol{K}_t(\boldsymbol{z}_t-\boldsymbol{H}_t\boldsymbol{\mu}_t^{-}) \\
\boldsymbol{\Sigma}_t&amp;=(\boldsymbol{I}-\boldsymbol{K}_t\boldsymbol{H}_t)\boldsymbol{\Sigma}_t^{-}
\end{eqnarray}" src="form_432.svg" width="1617" height="75"/>
</p>
<p> we can use the Kalman filter as the optimal Bayesian estimator assuming that all uncertainties are modeled by multivariate Gaussians. This is visualized in the following figure as well.</p>
<div class="image">
<img src="KalmanFilterBasics_BayesianwithGaussians.png" alt="" width="760"/>
<div class="caption">
Kalman filter as Bayesing estimator based on Gaussians</div></div>
    <h2><a class="anchor" id="KalmanFilterBasics-SQR"></a>
Square root implementation of the Kalman filter</h2>
<p>There are many reasons, why the covariance matrix of Kalman Filter can become singular. Mostly very small variances of the process noise or measurement noise, large differences in the magnitude of the state parameters or generally weakly conditions observation conditions, lead to singular or close-to singular matrices. In order to overcome or slightly improve such conditions, it is possible to describe the Kalman filter in its so-called square root form. Therefore it is important to understand how the square root of a matrix can be computed. Basically there are the following two possibilites for defining the square root of a matrix <img class="formulaInl" alt="$\boldsymbol{A}$" src="form_442.svg" width="16" height="16"/> <a class="anchor" id="eq-SQR01"></a> </p><p class="formulaDsp">
<img class="formulaDsp" alt="\begin{equation} \label{eq:eq-SQR01}
\boldsymbol{A} = \sqrt{\boldsymbol{A}}^T \sqrt{\boldsymbol{A}} \quad \text{resp.} \quad \boldsymbol{A^2} = \boldsymbol{A}^T \boldsymbol{A}
\end{equation}" src="form_443.svg" width="1607" height="26"/>
</p>
<p> Either of the two definitions can be achieved by the following three matrix decomposition methods</p><ul>
<li>Cholesky-Decomposition <a class="anchor" id="eq-SQR02"></a> <p class="formulaDsp">
<img class="formulaDsp" alt="\begin{equation} \label{eq:eq-SQR02}
\boldsymbol{A} = \boldsymbol{\Gamma}_{\boldsymbol{A}}^T \boldsymbol{\Gamma}_{\boldsymbol{A}}
\end{equation}" src="form_444.svg" width="1516" height="23"/>
</p>
</li>
<li>QR-Decomposition <a class="anchor" id="eq-SQR03"></a> <p class="formulaDsp">
<img class="formulaDsp" alt="\begin{eqnarray} \label{eq:eq-SQR03}
\boldsymbol{A} = \boldsymbol{Q} \boldsymbol{R} \\
\boldsymbol{A}^2 =  \boldsymbol{R}^T \boldsymbol{Q}^T  \boldsymbol{Q}\boldsymbol{R}  = \boldsymbol{R}^T \boldsymbol{R} \\
\boldsymbol{R} = \boldsymbol{\Gamma}_{\boldsymbol{A}^2}
\end{eqnarray}" src="form_446.svg" width="1587" height="72"/>
</p>
</li>
<li>SVD-Decomposition <a class="anchor" id="eq-SQR04"></a> <p class="formulaDsp">
<img class="formulaDsp" alt="\begin{eqnarray} \label{eq:eq-SQR04}
\boldsymbol{A} = \boldsymbol{Q} \boldsymbol{\Lambda}\boldsymbol{Q}^T   \\
\boldsymbol{A}^2 =  \boldsymbol{Q} \boldsymbol{\Lambda}^{1/2} \boldsymbol{\Lambda}^{1/2} \boldsymbol{Q}^T  \\
 \sqrt{\boldsymbol{A}}  = \boldsymbol{Q} \boldsymbol{\Lambda}^{1/2}
\end{eqnarray}" src="form_449.svg" width="1570" height="79"/>
</p>
 We (and INSTINCT) will make use of the QR decomposition in order to handle the Kalman filter in square root form. However, for the initialization of the filter, one needs to compute the square root of the initial covariance matrix. This is happening via a simple Cholesky decompositon. After that, only QR decompositions of the type <a class="anchor" id="eq-SQR05"></a> <p class="formulaDsp">
<img class="formulaDsp" alt="\begin{equation} \label{eq:eq-SQR05}
\boldsymbol{Q}, \boldsymbol{R} = qr\left(
\begin{bmatrix}
 \sqrt{\boldsymbol{A}} \\
  \sqrt{\boldsymbol{B}}
\end{bmatrix}
 \right)
\end{equation}" src="form_451.svg" width="1551" height="44"/>
</p>
 are needed. If we define the operator <img class="formulaInl" alt="$qr_R (\cdot,\cdot)$" src="form_452.svg" width="58" height="19"/> which returns us the <img class="formulaInl" alt="$\boldsymbol{R}$" src="form_453.svg" width="16" height="14"/> matrix of the QR decomposition, we can write <a class="anchor" id="eq-SQR06"></a> <p class="formulaDsp">
<img class="formulaDsp" alt="\begin{equation} \label{eq:eq-SQR06}
\boldsymbol{R} = qr_R ( \sqrt{\boldsymbol{A}},\sqrt{\boldsymbol{B}})
\end{equation}" src="form_454.svg" width="1545" height="23"/>
</p>
 If can be easily proven that <img class="formulaInl" alt="$\boldsymbol{R}$" src="form_453.svg" width="16" height="14"/> represents <a class="anchor" id="eq-SQR07"></a> <p class="formulaDsp">
<img class="formulaDsp" alt="\begin{equation} \label{eq:eq-SQR07}
\boldsymbol{R} = \sqrt{\boldsymbol{A} +\boldsymbol{B}}
\end{equation}" src="form_455.svg" width="1524" height="21"/>
</p>
</li>
</ul>
<p>If we start with the prediction step <a class="anchor" id="eq-SQR08"></a> </p><p class="formulaDsp">
<img class="formulaDsp" alt="\begin{equation} \label{eq:eq-SQR08}
\boldsymbol{\Sigma}_t^- =\boldsymbol{\Phi}_t\boldsymbol{\Sigma}_{t-1}^+\boldsymbol{\Phi}^T_t+\boldsymbol{Q}_t
\end{equation}" src="form_457.svg" width="1561" height="23"/>
</p>
<p> We can write this (assuming we have Cholesky decomposed the covariance matrix and the process noise matrix) as <a class="anchor" id="eq-SQR09"></a> </p><p class="formulaDsp">
<img class="formulaDsp" alt="\begin{equation} \label{eq:eq-SQR09}
\boldsymbol{\Gamma}^T_{\boldsymbol{\Sigma}_t^-} \boldsymbol{\Gamma}_{\boldsymbol{\Sigma}_t^-}
=\boldsymbol{\Phi}_t \boldsymbol{\Gamma}^T_{\boldsymbol{\Sigma}_{t-1}^+} \boldsymbol{\Gamma}_{\boldsymbol{\Sigma}_{t-1}^+}  \boldsymbol{\Phi}^T_t+
\boldsymbol{\Gamma}^T_{\boldsymbol{Q}_t} \boldsymbol{\Gamma}_{\boldsymbol{Q}_t}
\end{equation}" src="form_460.svg" width="1628" height="30"/>
</p>
<p> This relation can also be written in the form <a class="anchor" id="eq-SQR10"></a> </p><p class="formulaDsp">
<img class="formulaDsp" alt="\begin{equation} \label{eq:eq-SQR10}
\boldsymbol{\Gamma}^T_{\boldsymbol{\Sigma}_t^-} \boldsymbol{\Gamma}_{\boldsymbol{\Sigma}_t^-} =
\begin{bmatrix}
  \boldsymbol{\Phi}_t\boldsymbol{\Gamma}^T_{\boldsymbol{\Sigma}_{t-1}^+} &amp; \boldsymbol{\Gamma}^T_{\boldsymbol{Q}_t}
\end{bmatrix}
\begin{bmatrix}
  \boldsymbol{\Gamma}_{\boldsymbol{\Sigma}_{t-1}^+}\boldsymbol{\Phi}_t^T  \\
   \boldsymbol{\Gamma}_{\boldsymbol{Q}_t}
\end{bmatrix}
\end{equation}" src="form_465.svg" width="1631" height="53"/>
</p>
<p> Thus, as shown before, we can use the QR decomposition to obtain <a class="anchor" id="eq-SQR11"></a> </p><p class="formulaDsp">
<img class="formulaDsp" alt="\begin{equation} \label{eq:eq-SQR11}
\boldsymbol{\Gamma}_{\boldsymbol{\Sigma}_t^-}  =  qr_R ( \boldsymbol{\Gamma}_{\boldsymbol{\Sigma}_{t-1}^+}\boldsymbol{\Gamma}^T_{\boldsymbol{Q}_t} ,  \boldsymbol{\Gamma}_{\boldsymbol{Q}_t})
\end{equation}" src="form_468.svg" width="1579" height="28"/>
</p>
<p>The update step is a little bit more complicated, as we need to compute the Kalman gain by the help of the innovation matrix. Thus, the latter is QR decomposed first <a class="anchor" id="eq-SQR12"></a> </p><p class="formulaDsp">
<img class="formulaDsp" alt="\begin{equation} \label{eq:eq-SQR12}
\boldsymbol{\Gamma}_{\boldsymbol{S}_t}  =  qr_R ( \boldsymbol{\Gamma}_{\boldsymbol{\Sigma}_{t}^-}\boldsymbol{H}_t^T ,  \boldsymbol{\Gamma}_{\boldsymbol{R}_t})
\end{equation}" src="form_470.svg" width="1570" height="28"/>
</p>
<p> where <img class="formulaInl" alt="$\boldsymbol{\Gamma}_{\boldsymbol{R}_t}$" src="form_471.svg" width="28" height="18"/> is the square root of the measurement noise matrix (note: since this matrix is usually a diagonal matrix this square root of it can be compute straightforward without Cholesky decomposition) One can now compute the Kalman gain as <a class="anchor" id="eq-SQR13"></a> </p><p class="formulaDsp">
<img class="formulaDsp" alt="\begin{equation} \label{eq:eq-SQR13}
\boldsymbol{K}_t   =
\left(
\boldsymbol{\Gamma}_{\boldsymbol{S}_t}^{-1} (\boldsymbol{\Gamma}_{\boldsymbol{S}_t}^{-1})^T
\boldsymbol{H}_t \boldsymbol{\Gamma}^T_{\boldsymbol{\Sigma}_t^-} \boldsymbol{\Gamma}_{\boldsymbol{\Sigma}_t^-}
\right)^T
\end{equation}" src="form_473.svg" width="1601" height="37"/>
</p>
<p> With that one can compute the updated (square root) covariance matrix as <a class="anchor" id="eq-SQR14"></a> </p><p class="formulaDsp">
<img class="formulaDsp" alt="\begin{equation} \label{eq:eq-SQR14}
\boldsymbol{\Gamma}_{\boldsymbol{\Sigma}_t^+}  =
qr_R (
\boldsymbol{\Gamma}_{\boldsymbol{\Sigma}_t^-}(\boldsymbol{I} -\boldsymbol{K}_t \boldsymbol{H}_t)^T,  \boldsymbol{\Gamma}_{\boldsymbol{R}_t} \boldsymbol{K}_t^T
)
\end{equation}" src="form_474.svg" width="1626" height="28"/>
</p>
<p> Prediction and update of the state works like in the standard Kalman filter, since all necessary matrices are available anyway.</p>
<p>Note: While the square root form of the Kalman filter provides more numerical stability, and better balancing of the matrices, this advantage is lost at the point where the user need to back-compute the covariances matrices from their square root equivalent. </p>
</div><!-- contents -->
</div><!-- doc-content -->
<!-- HTML footer for doxygen 1.9.2-->
<!-- start footer part -->
<div id="nav-path" class="navpath">
    <!-- id is needed for treeview function! -->
    <ul>
        <li class="footer">Generated by <a href="https://www.doxygen.org/index.html"><img class="footer"
                    src="doxygen.svg" width="104" height="31" alt="doxygen" /></a> 1.9.8 </li>
    </ul>
</div>
</body>
</html>
